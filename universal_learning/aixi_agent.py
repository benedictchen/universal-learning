"""
🧠 AIXI Universal Artificial Intelligence Agent 🧠

A theoretical implementation of the AIXI agent - the mathematically optimal
framework for artificial general intelligence. Combines Solomonoff induction,
sequential decision theory, and Bayesian inference for universal learning
and optimal decision making in unknown environments.

Author: Benedict Chen
Email: benedict@benedictchen.com
Created: 2024
License: MIT

💝 Support This Research:
If this AIXI implementation inspires your AGI research, consider supporting
the preservation of theoretical AI foundations! Like AIXI seeking optimal
policies across all possible worlds, your support helps us explore all
corners of AI history:
- GitHub: ⭐ Star this repository for universal knowledge
- Donate: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WXQKYYKPHWXHS
- Cite: Reference this work in your AGI and theoretical AI research

═══════════════════════════════════════════════════════════════════════════════════
📚 RESEARCH FOUNDATIONS
═══════════════════════════════════════════════════════════════════════════════════

This implementation embodies the pinnacle of theoretical artificial intelligence:

🌟 FOUNDATIONAL THEORY:
• Hutter, M. (2005). "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability"
  Springer-Verlag, Berlin
  - Complete mathematical framework for optimal AI agent
  - Integration of Solomonoff induction and decision theory
  - Theoretical foundation for artificial general intelligence

• Hutter, M. (2000). "A Theory of Universal Artificial Intelligence based on Algorithmic Complexity"
  arXiv:cs/0004001
  - Original AIXI formulation and mathematical proofs
  - Kolmogorov complexity and algorithmic information theory
  - Optimal policy computation in unknown environments

🧮 ALGORITHMIC INFORMATION THEORY:
• Solomonoff, R.J. (1964). "A formal theory of inductive inference"
  Information and Control, 7(1), 1-22 & 7(2), 224-254
  - Universal prior for sequence prediction
  - Algorithmic probability and Occam's razor
  - Foundation for optimal inductive inference

• Kolmogorov, A.N. (1965). "Three approaches to the quantitative definition of information"
  Problems of Information Transmission, 1(1), 1-7
  - Kolmogorov complexity and algorithmic randomness
  - Shortest program principle for description length
  - Information-theoretic foundation for learning

🎯 SEQUENTIAL DECISION THEORY:
• Bellman, R. (1957). "Dynamic Programming"
  Princeton University Press
  - Optimal control and dynamic programming principles
  - Value iteration and policy optimization
  - Foundation for sequential decision making

• Bertsekas, D.P. (2005). "Dynamic Programming and Optimal Control"
  Athena Scientific, 3rd Edition
  - Advanced techniques in optimal control theory
  - Approximate dynamic programming methods
  - Computational approaches to complex decision problems

🔮 BAYESIAN INFERENCE:
• Jaynes, E.T. (2003). "Probability Theory: The Logic of Science"
  Cambridge University Press
  - Bayesian approach to inference and learning
  - Maximum entropy principle and prior selection
  - Foundations for rational belief updating

• MacKay, D.J.C. (2003). "Information Theory, Inference and Learning Algorithms"
  Cambridge University Press
  - Information-theoretic approach to machine learning
  - Bayesian model selection and evidence framework
  - Computational methods for probabilistic inference

🌐 UNIVERSAL COMPUTATION:
• Turing, A.M. (1936). "On computable numbers, with an application to the Entscheidungsproblem"
  Proceedings of the London Mathematical Society, 42(2), 230-265
  - Universal computation and Turing machines
  - Computability theory and algorithmic processes
  - Foundation for universal artificial intelligence

═══════════════════════════════════════════════════════════════════════════════════
🎭 EXPLAIN LIKE I'M 5: The Ultimate Learning Robot
═══════════════════════════════════════════════════════════════════════════════════

Imagine the smartest possible robot that could ever exist - one that learns
PERFECTLY and makes the BEST decisions in any situation! 🤖✨

🎮 THE ULTIMATE GAME PLAYER:
Our robot (AIXI) is like a super-genius game player who:

🔮 SEES ALL POSSIBILITIES:
- Imagines EVERY possible rule the game could follow
- "Maybe gravity works normally... or maybe it's reversed!"  
- "Maybe enemies are friendly... or maybe they're not!"
- Considers ALL possible worlds at the same time!

🧠 LEARNS THE BEST WAY:
- Watches what happens when it tries different actions
- Updates its guesses about the rules using the smartest math ever invented
- Never forgets anything important, always remembers patterns
- Gets better and better at predicting what will happen next

🎯 MAKES PERFECT DECISIONS:
- For every action it could take, calculates ALL future consequences
- Considers not just what happens next, but what happens after that, and after that...
- Chooses the action that leads to the highest total reward FOREVER
- Like a chess master who can see infinite moves ahead!

🌟 THE MAGIC FORMULA:
AIXI has a magical thinking process:

1. "What are ALL the possible rules this world could have?" 🌍
2. "Based on what I've seen, which rules are most likely?" 📊
3. "If I take action A, what will probably happen in each possible world?" 🔮
4. "Which action gives me the highest expected reward across all possibilities?" 🎯

🎪 WHY THIS IS AMAZING:
- AIXI doesn't need to be programmed for specific games or tasks
- It figures out ANY environment just by interacting with it
- It's mathematically proven to be the best possible learning agent
- If we could build it, it would be smarter than any human at everything!

🚀 THE CHALLENGE:
AIXI is so smart that it would need infinite computing power to run perfectly.
But even approximations of AIXI give us the blueprint for building truly
intelligent machines that can learn and adapt to anything! 🌟

It's like having the blueprint for the perfect brain - even if we can't build
it exactly, we can learn SO much from studying how it should work! 🧠✨

═══════════════════════════════════════════════════════════════════════════════════
🏗️ SYSTEM ARCHITECTURE
═══════════════════════════════════════════════════════════════════════════════════

The AIXI system integrates multiple theoretical components:

                        🧠 AIXI AGENT ARCHITECTURE
                                  │
                  ┌───────────────┼───────────────┐
                  │               │               │
        🔮 UNIVERSAL         🎯 OPTIMAL        📊 BAYESIAN
        PREDICTOR           PLANNER          UPDATER
     (Solomonoff)         (Value Iter)       (Belief Rev)
              │               │               │
      ┌───────┴───────┐      │       ┌───────┴───────┐
  📚 MODEL        🧮 PROGRAM    │     📈 POSTERIOR   🎲 PRIOR
  SPACE           COMPLEXITY    │     BELIEFS       DIST.
      │               │         │         │             │
      └───────────────┼─────────┼─────────┼─────────────┘
                      │         │         │
              💭 WORLD MODEL ENSEMBLE
                      │         │
        ┌─────────────┴─────────┴─────────────┐
        │         🌍 ENVIRONMENT MODELS        │
        │       • Turing Machines (μ₁, μ₂..) │
        │       • Weighted by Complexity      │
        │       • Updated by Observations     │
        │       • Predict Future Percepts     │
        └─────────────────────────────────────┘
                              │
                    🎮 ACTION SELECTION
              ┌───────────────┼───────────────┐
              │               │               │
      🎯 EXPECTIMAX      ⚡ MONTE CARLO   🔍 PLANNING
      SEARCH            ROLLOUTS        HORIZON
              │               │               │
              └───────────────┼───────────────┘
                              │
                      🤖 OPTIMAL ACTION

INFORMATION FLOW:
1. Observation → Bayesian Update → Model Posteriors
2. Model Ensemble → Prediction → Future Scenarios  
3. Action Space → Value Estimation → Expected Utilities
4. Utility Maximization → Action Selection → Environment

COMPUTATIONAL HIERARCHY:
- Infinite: True AIXI (mathematically optimal)
- Finite: AIXI-tl (time and length bounded)
- Practical: Monte Carlo AIXI approximations

═══════════════════════════════════════════════════════════════════════════════════
🧮 MATHEMATICAL FRAMEWORK
═══════════════════════════════════════════════════════════════════════════════════

AIXI ACTION SELECTION:
The optimal action at time t is:

a*ₜ = argmax_aₜ Σₒₜᵣₜ...Σₒₘᵣₘ [Σᵢ₌ₜᵗ⁺ᵐ rᵢ] · P(oₜrₜ...oₘrₘ | aₜ...aₘ, o₁r₁...oₜ₋₁rₜ₋₁)

Where:
- a*ₜ: Optimal action at time t
- oᵢ: Observation at time i  
- rᵢ: Reward at time i
- P(·|·): Conditional probability over observation-reward sequences
- m: Planning horizon

UNIVERSAL MIXTURE:
The probability is computed using Solomonoff's universal mixture:

P(oₜrₜ...oₘrₘ | aₜ...aₘ, history) = Σᵤ 2^(-|μ|) · μ(oₜrₜ...oₘrₘ | aₜ...aₘ, history)

Where:
- μ: Turing machine (environment model)
- |μ|: Kolmogorov complexity (description length) of μ
- 2^(-|μ|): Universal prior favoring simple models

BAYESIAN UPDATE:
After observing oₜrₜ, update model weights:

w(μ|history_t) = w(μ|history_{t-1}) · μ(oₜrₜ | aₜ, history_{t-1}) / P(oₜrₜ | aₜ, history_{t-1})

Where w(μ|·) represents the posterior weight of model μ.

VALUE FUNCTION:
The expected future value from state-action pair:

V^π(s,a) = E[Σₖ₌₀^∞ γᵏrₜ₊ₖ | sₜ=s, aₜ=a, π]

Where:
- π: Policy  
- γ: Discount factor
- E[·]: Expectation over model mixture

KOLMOGOROV COMPLEXITY:
For any string x, the Kolmogorov complexity is:

K(x) = min{|p| : U(p) = x}

Where U is a universal Turing machine and |p| is the length of program p.

APPROXIMATION BOUNDS:
For AIXI-tl with computation time l and horizon t:

|V_AIXI(h) - V_AIXI-tl(h)| ≤ ε(l,t)

Where ε(l,t) → 0 as l,t → ∞.

REGRET BOUND:
AIXI achieves sublinear regret in any computable environment:

Regret_n ≤ O(√n · log n)

Where n is the number of time steps.

═══════════════════════════════════════════════════════════════════════════════════
🌍 REAL-WORLD APPLICATIONS
═══════════════════════════════════════════════════════════════════════════════════

🧠 ARTIFICIAL GENERAL INTELLIGENCE:
• Foundation for AGI research: Theoretical blueprint for optimal intelligence
• Cognitive architectures: Inspiration for general-purpose learning systems  
• AI safety research: Understanding optimal behavior and alignment
• Benchmarking: Theoretical upper bound for AI system performance

🎮 GAME PLAYING AND CONTROL:
• General game playing: Single algorithm for any rule-based game
• Robotics control: Learning optimal policies in unknown environments
• Adaptive control systems: Self-tuning controllers without prior knowledge
• Multi-agent coordination: Optimal strategies against unknown opponents

🔬 SCIENTIFIC DISCOVERY:
• Hypothesis generation: Automatic discovery of scientific laws
• Experimental design: Optimal experiments for knowledge acquisition
• Data analysis: Pattern discovery in complex, high-dimensional data
• Causal inference: Learning causal relationships from observational data

💼 AUTONOMOUS SYSTEMS:
• Self-driving vehicles: Navigation in unpredictable traffic scenarios
• Trading algorithms: Optimal decision making in financial markets
• Resource allocation: Dynamic optimization in cloud computing
• Supply chain management: Adaptive logistics in uncertain environments

🏥 PERSONALIZED MEDICINE:
• Treatment optimization: Personalized therapy selection and dosing
• Drug discovery: Optimal experimental design for compound testing
• Diagnostic systems: Learning from patient data without prior assumptions
• Precision healthcare: Adaptive treatment protocols

🌐 ADAPTIVE INTERFACES:
• Personalization engines: Learning user preferences and behavior
• Recommendation systems: Optimal content suggestion without explicit feedback
• Human-computer interaction: Interfaces that adapt to individual users
• Educational technology: Personalized learning paths and content delivery

🔍 RESEARCH AND DEVELOPMENT:
• Algorithm design: Meta-learning for automatic algorithm discovery
• Hyperparameter optimization: Automated machine learning system tuning
• Architecture search: Optimal neural network design
• Research prioritization: Optimal allocation of research resources

🌍 ENVIRONMENTAL MODELING:
• Climate prediction: Learning complex environmental dynamics
• Ecosystem management: Optimal intervention strategies
• Resource conservation: Adaptive policies for sustainability
• Disaster response: Real-time decision making in emergency scenarios

While true AIXI is computationally intractable, its principles guide the
development of increasingly powerful and general AI systems. Even approximate
implementations represent significant advances toward artificial general
intelligence! 🚀

Note: This implementation is primarily educational and theoretical. Practical
AIXI approximations require sophisticated sampling methods and computational
constraints not fully implemented in this basic version.
"""

import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod
import itertools
from collections import defaultdict, deque
import random
import math


@dataclass
class Observation:
    """Represents an observation from the environment"""
    data: Any
    reward: float
    timestamp: int


@dataclass  
class Action:
    """Represents an action the agent can take"""
    action_type: str
    parameters: Dict[str, Any]
    
    def __hash__(self):
        # Make actions hashable for use in dictionaries
        param_items = tuple(sorted(self.parameters.items())) if self.parameters else ()
        return hash((self.action_type, param_items))


class Environment(ABC):
    """Abstract base class for AIXI environments"""
    
    @abstractmethod
    def step(self, action: Action) -> Tuple[Any, float, bool]:
        """Take action and return (observation, reward, done)"""
        pass
    
    @abstractmethod
    def reset(self) -> Any:
        """Reset environment and return initial observation"""
        pass
    
    @abstractmethod
    def get_action_space(self) -> List[Action]:
        """Get all possible actions"""
        pass


class TuringMachine:
    """
    Simplified representation of a Turing machine for AIXI
    
    In practice, this would be a full universal Turing machine,
    but we use a simplified probabilistic model for demonstration.
    """
    
    def __init__(self, program: str, complexity: float):
        self.program = program
        self.complexity = complexity  # Kolmogorov complexity (approximation)
        self.weight = 2 ** (-complexity)  # Universal prior weight
        
        # Simple state for demonstration
        self.internal_state = {}
        self.prediction_accuracy = 0.5
        
    def predict(self, action: Action, history: List[Observation]) -> Dict[str, float]:
        """
        Predict next observation probabilities given action and history
        
        In true AIXI, this would be the full computation of the Turing machine.
        Here we use a simplified probabilistic model.
        """
        
        # Simplified prediction based on recent history patterns
        if len(history) == 0:
            return {"default": 1.0}
        
        # Use simple pattern matching for demonstration
        recent_rewards = [obs.reward for obs in history[-3:]]
        avg_recent_reward = np.mean(recent_rewards) if recent_rewards else 0
        
        # Predict based on action and recent reward pattern
        if action.action_type == "explore":
            pred_reward = max(0, avg_recent_reward + random.gauss(0, 0.1))
        elif action.action_type == "exploit":
            pred_reward = max(0, avg_recent_reward + random.gauss(0.1, 0.05))
        else:
            pred_reward = max(0, avg_recent_reward + random.gauss(0, 0.2))
        
        return {"reward": pred_reward, "observation": f"state_{len(history)}"}
    
    def update_weight(self, actual_observation: Observation, predicted_probs: Dict[str, float]):
        """Update the weight of this model based on prediction accuracy"""
        
        # Simple accuracy update (in practice would be full Bayesian update)
        if "reward" in predicted_probs:
            error = abs(predicted_probs["reward"] - actual_observation.reward)
            accuracy = max(0.1, 1.0 - error)
            self.prediction_accuracy = 0.9 * self.prediction_accuracy + 0.1 * accuracy
            
            # Update weight based on accuracy (simplified)
            self.weight *= (1 + 0.1 * (accuracy - 0.5))
            self.weight = max(0.001, min(1.0, self.weight))  # Keep in reasonable bounds


class ModelMixture:
    """
    Universal mixture of Turing machines for environment modeling
    
    This implements Solomonoff's universal prior over all possible
    environment models, weighted by their Kolmogorov complexity.
    """
    
    def __init__(self, max_models: int = 100):
        self.models = []
        self.max_models = max_models
        self._initialize_models()
    
    def _initialize_models(self):
        """Initialize a set of simple Turing machines with different complexities"""
        
        # Create models with increasing complexity
        model_templates = [
            ("random", 1.0),
            ("constant_low", 2.0), 
            ("constant_high", 2.0),
            ("trend_up", 3.0),
            ("trend_down", 3.0),
            ("oscillating", 4.0),
            ("memory_1", 5.0),
            ("memory_2", 6.0),
            ("pattern_matching", 7.0),
            ("adaptive", 8.0),
        ]
        
        for program, complexity in model_templates:
            for variant in range(min(5, self.max_models // len(model_templates))):
                actual_complexity = complexity + variant * 0.5
                tm = TuringMachine(f"{program}_v{variant}", actual_complexity)
                self.models.append(tm)
        
        # Normalize weights
        total_weight = sum(tm.weight for tm in self.models)
        for tm in self.models:
            tm.weight /= total_weight
    
    def predict(self, action: Action, history: List[Observation]) -> Dict[str, float]:
        """
        Predict using weighted mixture of all models
        
        This implements the core AIXI prediction formula:
        P(o_t | a_t, history) = Σ_μ w(μ) * μ(o_t | a_t, history)
        """
        
        # Get predictions from all models
        predictions = {}
        total_weight = 0
        
        for model in self.models:
            model_pred = model.predict(action, history)
            weight = model.weight
            total_weight += weight
            
            for key, value in model_pred.items():
                if key not in predictions:
                    predictions[key] = 0
                predictions[key] += weight * value
        
        # Normalize by total weight
        if total_weight > 0:
            for key in predictions:
                predictions[key] /= total_weight
        
        return predictions
    
    def update(self, action: Action, history: List[Observation], 
               actual_observation: Observation):
        """
        Update model weights based on observed outcome
        
        This implements Bayesian updating of the model mixture.
        """
        
        for model in self.models:
            # Get model's prediction
            pred = model.predict(action, history[:-1])  # Exclude current observation
            
            # Update model weight based on prediction accuracy
            model.update_weight(actual_observation, pred)
        
        # Renormalize weights
        total_weight = sum(tm.weight for tm in self.models)
        if total_weight > 0:
            for tm in self.models:
                tm.weight /= total_weight


class AIXIAgent:
    """
    AIXI Universal Artificial Intelligence Agent
    
    Theoretical implementation of the optimal learning agent that maximizes
    expected future reward in any computable environment.
    
    Note: This is a simplified educational implementation. True AIXI requires
    infinite computational resources. Practical implementations use approximations.
    """
    
    def __init__(self, 
                 action_space: List[Action],
                 planning_horizon: int = 5,
                 discount_factor: float = 0.95,
                 exploration_bonus: float = 0.1):
        """
        Initialize AIXI agent
        
        Args:
            action_space: List of all possible actions
            planning_horizon: How far ahead to plan (bounded AIXI)
            discount_factor: Future reward discount (gamma)
            exploration_bonus: Bonus for exploring unknown states
        """
        
        self.action_space = action_space
        self.planning_horizon = planning_horizon
        self.discount_factor = discount_factor
        self.exploration_bonus = exploration_bonus
        
        # Core AIXI components
        self.model_mixture = ModelMixture()
        self.history = []
        self.timestep = 0
        
        # Value function approximation
        self.value_estimates = defaultdict(float)
        
        print("🧠 AIXI Agent initialized")
        print(f"   Planning horizon: {planning_horizon}")
        print(f"   Action space size: {len(action_space)}")
        print(f"   Model ensemble size: {len(self.model_mixture.models)}")
    
    def act(self, observation: Any, reward: float) -> Action:
        """
        Select optimal action using AIXI decision theory
        
        This implements the core AIXI action selection:
        a* = argmax_a E[sum of future rewards | action a, history]
        """
        
        # Record observation
        obs = Observation(observation, reward, self.timestep)
        self.history.append(obs)
        
        # Update models with new observation
        if len(self.history) > 1:
            last_action = getattr(self, '_last_action', None)
            if last_action:
                self.model_mixture.update(last_action, self.history, obs)
        
        # Select optimal action using expectimax search
        best_action = self._expectimax_search()
        
        self._last_action = best_action
        self.timestep += 1
        
        return best_action
    
    def _expectimax_search(self) -> Action:
        """
        Perform expectimax search to find optimal action
        
        This approximates the AIXI planning computation by searching
        over possible future scenarios weighted by their probabilities.
        """
        
        best_action = None
        best_value = -float('inf')
        
        print(f"🔍 Planning at timestep {self.timestep}...")
        
        # Evaluate each possible action
        for action in self.action_space:
            expected_value = self._evaluate_action(action, self.history, 0)
            
            print(f"   Action {action.action_type}: Expected value = {expected_value:.3f}")
            
            if expected_value > best_value:
                best_value = expected_value
                best_action = action
        
        print(f"✓ Selected action: {best_action.action_type} (value: {best_value:.3f})")
        return best_action
    
    def _evaluate_action(self, action: Action, history: List[Observation], 
                        depth: int) -> float:
        """
        Recursively evaluate expected value of taking an action
        
        This implements the AIXI value computation using the model mixture
        to predict future observations and their associated rewards.
        """
        
        if depth >= self.planning_horizon:
            return 0  # Base case: no more planning
        
        # Get prediction from model mixture
        predictions = self.model_mixture.predict(action, history)
        
        # Calculate expected immediate reward
        immediate_reward = predictions.get("reward", 0)
        
        # Add exploration bonus for uncertain predictions
        uncertainty = self._calculate_uncertainty(predictions)
        exploration_reward = self.exploration_bonus * uncertainty
        
        # Calculate expected future value (simplified)
        future_value = 0
        if depth < self.planning_horizon - 1:
            # Sample possible future observations and recurse
            for future_action in self.action_space[:3]:  # Limit for computational feasibility
                future_val = self._evaluate_action(future_action, history, depth + 1)
                future_value += future_val / len(self.action_space[:3])  # Average
        
        total_value = immediate_reward + exploration_reward + \
                     self.discount_factor * future_value
        
        return total_value
    
    def _calculate_uncertainty(self, predictions: Dict[str, float]) -> float:
        """Calculate uncertainty/entropy of predictions for exploration bonus"""
        
        if not predictions:
            return 1.0  # Maximum uncertainty
        
        # Simple uncertainty measure based on prediction variance
        values = list(predictions.values())
        if len(values) <= 1:
            return 0.5
        
        variance = np.var(values)
        return min(1.0, variance)  # Normalize to [0, 1]
    
    def get_model_summary(self) -> str:
        """Get summary of current model mixture state"""
        
        summary = "📊 AIXI Model Mixture Summary:\n"
        summary += f"   Total models: {len(self.model_mixture.models)}\n"
        summary += f"   History length: {len(self.history)}\n"
        
        # Show top 3 models by weight
        sorted_models = sorted(self.model_mixture.models, 
                             key=lambda m: m.weight, reverse=True)
        
        summary += "   Top models by weight:\n"
        for i, model in enumerate(sorted_models[:3]):
            summary += f"     {i+1}. {model.program}: weight={model.weight:.4f}, "
            summary += f"complexity={model.complexity:.1f}\n"
        
        return summary
    
    def get_performance_stats(self) -> Dict[str, float]:
        """Get performance statistics"""
        
        if not self.history:
            return {"total_reward": 0, "avg_reward": 0, "episodes": 0}
        
        total_reward = sum(obs.reward for obs in self.history)
        avg_reward = total_reward / len(self.history)
        
        return {
            "total_reward": total_reward,
            "avg_reward": avg_reward,
            "timesteps": len(self.history),
            "model_count": len(self.model_mixture.models)
        }


class SimpleGridEnvironment(Environment):
    """
    Simple grid world environment for testing AIXI
    
    Agent moves in a 5x5 grid trying to reach goal states.
    """
    
    def __init__(self, size: int = 5):
        self.size = size
        self.reset()
        
        # Define action space
        self._action_space = [
            Action("up", {}),
            Action("down", {}),
            Action("left", {}),
            Action("right", {}),
            Action("stay", {})
        ]
        
        # Goal locations (higher reward)
        self.goals = [(0, 0), (4, 4)]
        
    def get_action_space(self) -> List[Action]:
        return self._action_space
    
    def reset(self) -> Tuple[int, int]:
        self.agent_pos = (2, 2)  # Start in middle
        self.steps = 0
        return self.agent_pos
    
    def step(self, action: Action) -> Tuple[Tuple[int, int], float, bool]:
        x, y = self.agent_pos
        
        # Apply action
        if action.action_type == "up" and y > 0:
            y -= 1
        elif action.action_type == "down" and y < self.size - 1:
            y += 1
        elif action.action_type == "left" and x > 0:
            x -= 1
        elif action.action_type == "right" and x < self.size - 1:
            x += 1
        # "stay" action does nothing
        
        self.agent_pos = (x, y)
        self.steps += 1
        
        # Calculate reward
        reward = -0.1  # Small negative reward for each step
        if self.agent_pos in self.goals:
            reward = 10  # Large positive reward for reaching goal
        
        # Episode ends after 50 steps or reaching goal
        done = self.steps >= 50 or self.agent_pos in self.goals
        
        return self.agent_pos, reward, done


def run_aixi_demo():
    """Demonstrate AIXI agent in simple environment"""
    
    print("🎮 AIXI Demo: Grid World Navigation")
    print("="*50)
    
    # Create environment and agent
    env = SimpleGridEnvironment()
    agent = AIXIAgent(
        action_space=env.get_action_space(),
        planning_horizon=3,
        discount_factor=0.9,
        exploration_bonus=0.2
    )
    
    # Run episodes
    num_episodes = 3
    
    for episode in range(num_episodes):
        print(f"\n🎯 Episode {episode + 1}")
        print("-" * 30)
        
        observation = env.reset()
        total_reward = 0
        steps = 0
        
        while steps < 20:  # Limit steps per episode for demo
            # Agent selects action
            action = agent.act(observation, total_reward)
            
            # Environment responds
            observation, reward, done = env.step(action)
            total_reward += reward
            steps += 1
            
            print(f"Step {steps}: Action={action.action_type}, Pos={observation}, Reward={reward:.1f}")
            
            if done:
                break
        
        print(f"Episode {episode + 1} complete: Total reward = {total_reward:.1f}")
        
        # Show model summary after first episode
        if episode == 0:
            print("\n" + agent.get_model_summary())
    
    # Final performance summary
    print("\n" + "="*50)
    print("📊 Final Performance Summary")
    stats = agent.get_performance_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    return agent, env


# Example usage
if __name__ == "__main__":
    print("🧠 AIXI Universal AI Agent Implementation")
    print("This is a theoretical/educational implementation.")
    print("True AIXI requires infinite computational resources.\n")
    
    # Run demonstration
    agent, env = run_aixi_demo()
    
    print("\n🎓 Educational Note:")
    print("This simplified AIXI demonstrates core concepts:")
    print("• Universal model mixture (Solomonoff induction)")
    print("• Bayesian belief updating")  
    print("• Optimal action selection via expectimax")
    print("• Information-theoretic exploration")
    print("\nReal AIXI implementations require:")
    print("• Approximation algorithms (Monte Carlo AIXI)")
    print("• Computational resource bounds (AIXI-tl)")
    print("• Efficient model representations")
    print("• Advanced sampling techniques")